{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cb0f03",
   "metadata": {},
   "source": [
    "#### Install required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95914aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!python3 -m pip install --upgrade Pillow\n",
    "!pip install tiffile\n",
    "!pip install qlty\n",
    "!pip install opencv-python\n",
    "!pip install connected-components-3d\n",
    "!pip install rdp \n",
    "!git clone https://github.com/phzwart/dlsia.git\n",
    "!cd dlsia && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed175044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import cc3d \n",
    "import csv\n",
    "import random\n",
    "import rdp \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from skimage import exposure,morphology\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import qlty\n",
    "from qlty import qlty2D\n",
    "\n",
    "from dlsia.core import helpers, train_scripts, corcoef\n",
    "from dlsia.core.networks import msdnet, tunet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b05801",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays ten random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "    n = 7\n",
    "\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    print('The indices of the images are ', indices)\n",
    "    images1 = array1[indices, :]\n",
    "    images2 = array2[indices, :]\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    \n",
    "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(image1, vmin=0, vmax=1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image2, vmin=0, vmax=1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def regression_metrics( preds, target):\n",
    "    tmp = corcoef.cc(preds.cpu().flatten(), target.cpu().flatten() )\n",
    "    return(tmp)\n",
    "\n",
    "\n",
    "def segment_imgs(testloader, net):\n",
    "    \"\"\" Modified for input and no ground truth\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    seg_imgs = []\n",
    "    noisy_imgs = []\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(testloader):\n",
    "            noisy = batch\n",
    "            noisy = noisy[0]\n",
    "            noisy = torch.FloatTensor(noisy)\n",
    "            noisy = noisy.to(device)\n",
    "            output = net(noisy)\n",
    "            if counter == 0:\n",
    "                seg_imgs = output.detach().cpu()\n",
    "                noisy_imgs = noisy.detach().cpu()\n",
    "            else:\n",
    "                seg_imgs = torch.cat((seg_imgs, output.detach().cpu()), 0)\n",
    "                noisy_imgs = torch.cat((noisy_imgs, noisy.detach().cpu()), 0)\n",
    "                \n",
    "            counter+=1\n",
    "            del output\n",
    "            del noisy\n",
    "            torch.cuda.empty_cache()\n",
    "    return seg_imgs, noisy_imgs\n",
    "\n",
    "def create_network(model_type, params):\n",
    "    # set model parameters and initialize the network\n",
    "    if model_type == \"SMSNet\":\n",
    "        net = SMSNet.random_SMS_network(**params)\n",
    "        model_params = {\n",
    "          \"in_channels\": net.in_channels,\n",
    "          \"out_channels\": net.out_channels,\n",
    "          \"in_shape\": net.in_shape,\n",
    "          \"out_shape\": net.out_shape,\n",
    "          \"scaling_table\": net.scaling_table,\n",
    "          \"network_graph\": net.network_graph,\n",
    "          \"channel_count\": net.channel_count,\n",
    "          \"convolution_kernel_size\": net.convolution_kernel_size,\n",
    "          \"first_action\": net.first_action,\n",
    "          \"hidden_action\": net.hidden_action,\n",
    "          \"last_action\":net.last_action,\n",
    "        }\n",
    "        return net, model_params\n",
    "    elif model_type == \"MSDNet\":\n",
    "        net = msdnet.MixedScaleDenseNetwork(**params)\n",
    "        return net, params\n",
    "    elif model_type == 'TUNet':\n",
    "        net = tunet.TUNet(**params)\n",
    "        return net, params\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def save_stack(imgx, imgy, imgz, d_type = None, return_stacks=True):\n",
    "    imgx_stack = []\n",
    "    imgy_stack = []\n",
    "    imgz_stack = []\n",
    "\n",
    "    for j in tqdm(range(len(imgx))):\n",
    "        ix = Image.open(imgx[j])\n",
    "        iy = Image.open(imgy[j])\n",
    "        iz = Image.open(imgz[j])\n",
    "        \n",
    "        ix.load()\n",
    "        iy.load()\n",
    "        iz.load()\n",
    "\n",
    "        if d_type == None:\n",
    "            ix = np.array(ix)\n",
    "            iy = np.array(iy)\n",
    "            iz = np.array(iz)\n",
    "        else:\n",
    "            ix = np.array(ix, dtype=d_type)\n",
    "            iy = np.array(iy, dtype=d_type)\n",
    "            iz = np.array(iz, dtype=d_type)\n",
    "\n",
    "        imgx_stack.append(ix)\n",
    "        imgy_stack.append(iy)\n",
    "        imgz_stack.append(iz)\n",
    "\n",
    "    imgx_stack = np.array(imgx_stack)\n",
    "    imgy_stack = np.array(imgy_stack)\n",
    "    imgz_stack = np.array(imgz_stack)\n",
    "        \n",
    "    if return_stacks == True:\n",
    "        return imgx_stack, imgy_stack, imgz_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to images to be segmented. Requires jpeg images \n",
    "source ='/data/FIBSEM/testing_scripts/sourceFile/images' \n",
    "# Path to trained Tunet, should have params.npy and a net file\n",
    "maindir= '/data/FIBSEM/testing_scripts/sourceFile/Results/tunet3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60482b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(source) if f.endswith('.jpg')]\n",
    "print('Number of files to segment: ', len(files))\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = []\n",
    "for file in files:\n",
    "    img = Image.open(f'{source}/{file}')\n",
    "    img.load()\n",
    "    img = np.array(img, dtype='float32')\n",
    "    # Uncomment this if images are RGB \n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    test_imgs.append(img)\n",
    "test_imgs = np.expand_dims(np.array(test_imgs), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f95d6",
   "metadata": {},
   "source": [
    "### The following qlty and pre processing code is same as train.ipynb. any changes made while training the network should also be made here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quilt = qlty2D.NCYXQuilt(X=test_imgs.shape[3],\n",
    "                         Y=test_imgs.shape[2],\n",
    "                         window=(256,256),\n",
    "                         step=(64,64),\n",
    "                         border=(10,10),\n",
    "                         border_weight=0)\n",
    "\n",
    "def imageSplit(quilt,test_imgs):\n",
    "    dicedImgs = []\n",
    "    labeled_imgs = torch.Tensor(test_imgs)\n",
    "    labeled_imgs = quilt.unstitch(labeled_imgs)\n",
    "    print(\"x shape: \",test_imgs.shape)\n",
    "    print(\"x_bits shape:\", labeled_imgs.shape)\n",
    "    \n",
    "    for i in range(len(labeled_imgs)):\n",
    "        bilateral = cv2.bilateralFilter(labeled_imgs[i][0].numpy(),5,50,10)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3)\n",
    "        bilateral= bilateral.astype(np.uint16)\n",
    "        final = clahe.apply(bilateral)\n",
    "        x = exposure.equalize_hist(final)\n",
    "        dicedImgs.append(x.astype(np.float32))\n",
    "        #dicedImgs.append(final.astype(np.float32))\n",
    "    return np.expand_dims(np.array(dicedImgs), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.load(maindir + '/params.npy', allow_pickle=True)\n",
    "params = params[0]\n",
    "print('The following define the network parameters: ', params)\n",
    "\n",
    "model_type = 'TUNet'\n",
    "#model_type = 'MSDNet'  \n",
    "\n",
    "net, model_params = create_network(model_type, params)\n",
    "net.load_state_dict(torch.load(maindir + '/net'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = helpers.get_device()\n",
    "device='cuda:1'\n",
    "print('Device we compute on: ', device)\n",
    "print('Number of parameters: ', helpers.count_parameters(net))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cbad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save the segmented images and masks \n",
    "folder = '/data/FIBSEM/testing_scripts/sourceFile/outputs'\n",
    "# mapper maps the label and feature. eg 1:\"filament\" represents label 1 in training which is marked as filament \n",
    "# this generates folder filament with  segmented images. {1:\"ribosomes\",2:\"tube\",3:\"mem\"}\n",
    "output_mapper = {1:\"filament\"}\n",
    "\n",
    "# Number of files to process in one batch. reduce or increase this based on available memory. \n",
    "file_batch = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b117f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_masks = None \n",
    "\n",
    "for k,v in output_mapper.items():\n",
    "    if os.path.isdir(f'{folder}/{v}/segments') is False: os.mkdir(f'{folder}/{v}/segments')\n",
    "    \n",
    "for i in range(200,220,file_batch):\n",
    "    imgs = test_imgs[i:i+file_batch]\n",
    "    dicedtestImgs = imageSplit(quilt, imgs)\n",
    "    \n",
    "    batch_size = file_batch\n",
    "    num_workers = 0    #increase to 1 or 2 with multiple GPUs\n",
    "    test_data = TensorDataset(torch.Tensor(dicedtestImgs))\n",
    "    test_loader_params = {'batch_size': batch_size,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "    test_loader = DataLoader(test_data, **test_loader_params)  \n",
    "    \n",
    "    output, input_imgs  = segment_imgs(test_loader, net)\n",
    "    stitched_output = quilt.stitch(torch.tensor(output))\n",
    "    o = torch.squeeze(stitched_output[0], 1)\n",
    "    tunet3_output = torch.argmax(o.cpu()[:,:,:,:].data, dim=1)\n",
    "    \n",
    "    masks=tunet3_output.numpy()\n",
    "    imgs= np.squeeze(imgs,1)\n",
    "    \n",
    "    out_masks=masks if out_masks is None else np.vstack((out_masks,masks))\n",
    "    \n",
    "    for k,v in output_mapper.items():\n",
    "        idx=(masks==k)\n",
    "        structures=np.zeros(imgs.shape)\n",
    "        structures[idx]=imgs[idx]\n",
    "        out_path = f'{folder}/{v}/segments/'\n",
    "        \n",
    "        for j in range(structures.shape[0]):\n",
    "            name = f'{i+j:03}.jpg'\n",
    "            print(out_path+name)\n",
    "            Image.fromarray(structures[j].astype(np.uint8)).save(out_path+name)\n",
    "        \n",
    "    del output\n",
    "    del tunet3_output\n",
    "    del input_imgs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "imwrite(folder+'/masks.tif', out_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a912d0",
   "metadata": {},
   "source": [
    "### Clean image segments \n",
    "This code cleans up small objects from the segmented images in the folder which are generated because of false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_size = 100 # remove objects smaller than this size. \n",
    "\n",
    "def clean_stack(img_stack, minim):\n",
    "        cleaned = np.copy(img_stack)\n",
    "        cleaned_index = (cleaned!=0)\n",
    "        for j in tqdm(range(len(cleaned))):\n",
    "            img = cleaned_index[j,:] \n",
    "            img = morphology.remove_small_objects(img, minim, connectivity=1)\n",
    "            target_img = cleaned[j,:,:]\n",
    "            cleaned[j,:,:] = np.multiply(target_img, img)\n",
    "        return cleaned\n",
    "\n",
    "for k,v in output_mapper.items():\n",
    "    path = f'{folder}/{v}/segments'\n",
    "        \n",
    "    files = []\n",
    "    for file in glob.glob(path+\"/*.jpg\"):files.append(file)\n",
    "    files = sorted(files)\n",
    "    imgs= []\n",
    "    for j in range(len(files)):\n",
    "        img = Image.open(files[j])\n",
    "        img.load()\n",
    "        img = np.array(img, dtype='float32')\n",
    "        imgs.append(img)\n",
    "    imwrite(f'{folder}/{v}/{v}.tiff', clean_stack(imgs, object_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd8aafe",
   "metadata": {},
   "source": [
    "### Generate Co-ordinates for subtomo averaging  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad25f26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def simplify_points(arr,z):\n",
    "    \n",
    "    final_coord = []\n",
    "    \n",
    "    count = collections.defaultdict(list)\n",
    "    rows,cols = np.nonzero(arr)\n",
    "    \n",
    "    for r,c in zip(list(rows),list(cols)):\n",
    "        pixel = arr[r][c]\n",
    "        count[pixel].append([r,c])\n",
    "        \n",
    "    for pixel, coord in count.items():\n",
    "        \n",
    "        simplied = rdp.rdp_iter(np.array(coord),epsilon=0.5)\n",
    "        simplied = [(x[0],x[1],z,pixel) for x in simplied]\n",
    "        final_coord += simplied \n",
    "    \n",
    "    return final_coord\n",
    "\n",
    "for k,v in output_mapper.items():\n",
    "    file =f'{folder}/{v}/{v}.tiff'\n",
    "    \n",
    "    imgs =imread(file)\n",
    "    imgs[imgs!=0]=1\n",
    "    labels_out= cc3d.connected_components(imgs, connectivity=6)\n",
    "    total,rows,cols = labels_out.shape\n",
    "\n",
    "    with open(f'{folder}/{v}/coordinates.csv','a',encoding='UTF8',newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for images in range(total):\n",
    "            for val in simplify_points(labels_out[images],images):\n",
    "                writer.writerow(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309c5ba",
   "metadata": {},
   "source": [
    "### Simplify co-ordinates \n",
    "Generate only one co-ordinate per feature(generates one co-ordinate for each filament or ribosome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{folder}/{v}/coordinates.csv',names=[\"x\",\"y\",\"z\",\"pixel\"])\n",
    "grouped = df.groupby('pixel')\n",
    "random_points = grouped.apply(lambda x: x.iloc[np.random.randint(0,len(x))])\n",
    "random_points.to_csv(f'{folder}/{v}/simplified_coord.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
